{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ee0642",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Report 1 CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d708b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd59ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "colnames=['title', 'text', 'label'] \n",
    "data = pd.read_csv(\"./news.csv\",usecols=colnames)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0b1207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking number of real vs fake data\n",
    "data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfcfe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning the data before making the wordclouds\n",
    "# remove the hashtags, mentions and unwanted characters.\n",
    "import re\n",
    "def  clean_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
    "    return df\n",
    "\n",
    "data_clean = clean_text(data, \"text\")\n",
    "data_clean = data_clean(data, \"title\")\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8682566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stop words\n",
    "import en_core_web_sm\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer, WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from string import punctuation\n",
    "\n",
    "nlp = en_core_web_sm.load() \n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop = set(stopwords.words('english'))\n",
    "punctuation = list(string.punctuation) #already taken care of with the cleaning function.\n",
    "stop.update(punctuation)\n",
    "w_tokenizer = WhitespaceTokenizer()\n",
    "def furnished(text):\n",
    "    final_text = []\n",
    "    for i in w_tokenizer.tokenize(text):\n",
    "       if i.lower() not in stop:\n",
    "           word = lemmatizer.lemmatize(i)\n",
    "           final_text.append(word.lower())\n",
    "    return \" \".join(final_text)\n",
    "\n",
    "data_clean.text = data_clean.text.apply(furnished)\n",
    "data_clean.text = data_clean.title.apply(furnished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87488dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate Real and Fake Data into different data frames\n",
    "data_clean_REAL=data_clean.loc[data_clean.label==\"REAL\"]\n",
    "data_clean_FAKE=data_clean.loc[data_clean.label==\"FAKE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc04be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Visual Word Cloud for REAL DATA text column\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "df = data_clean_REAL\n",
    "comment_words = ''\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "for val in df.text:\n",
    "    val = str(val)\n",
    "    tokens = val.split()\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower() \n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    "  \n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 10).generate(comment_words)\n",
    "  \n",
    "# plot the WordCloud                   \n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e6b262",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Visual Word Cloud for FAKE DATA text column\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "df = data_clean_FAKE\n",
    "comment_words = ''\n",
    "stopwords = set(STOPWORDS)\n",
    "  \n",
    "for val in df.text:\n",
    "    val = str(val)\n",
    "    tokens = val.split()\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    "  \n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 10).generate(comment_words)\n",
    "  \n",
    "# plot the WordCloud                     \n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229fa22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Visual Word Cloud for REAL DATA title column\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "df = data_clean_REAL\n",
    "comment_words = ''\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "for val in df.title:\n",
    "    val = str(val)\n",
    "    tokens = val.split()\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    "  \n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 10).generate(comment_words)\n",
    "  \n",
    "# plot the WordCloud                      \n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5689526",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Visual Word Cloud for FAKE DATA title column\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "df = data_clean_FAKE\n",
    "comment_words = ''\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "for val in df.title:\n",
    "    val = str(val)\n",
    "    tokens = val.split()\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    "  \n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 10).generate(comment_words)\n",
    "  \n",
    "# plot the WordCloud                     \n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fcc37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Repot 1 Plotting lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfe0c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from matplotlib.ticker import StrMethodFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dff46c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "colnames=['title', 'text', 'label'] \n",
    "data = pd.read_csv(\"./news.csv\",usecols=colnames)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091e0747",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new columns with the string and word lengths for the text and titles\n",
    "data[\"title length\"] = data.title.str.len()\n",
    "data[\"text length\"] = data.text.str.len()\n",
    "data['Class'] = np.where(data['label'] ==\"FAKE\", 0, 1)\n",
    "data['text words len']=data.text.str.split().str.len()\n",
    "data['title words len']=data.title.str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af0bd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting title length vs text length\n",
    "x = data[\"text length\"]\n",
    "y = data[\"title length\"]\n",
    "label = data[\"Class\"]\n",
    "colors = ['red','green']\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.scatter(x, y, c=label, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "plt.title(\"title length & text length vs. class\")\n",
    "plt.xlabel(\"text length\")\n",
    "plt.ylabel(\"title length\")\n",
    "\n",
    "cb = plt.colorbar()\n",
    "loc = np.arange(0,max(label),max(label)/float(len(colors)))\n",
    "cb.set_ticks(loc)\n",
    "cb.set_ticklabels(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f75a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping outliers\n",
    "data = data.drop(data[data[\"text length\"] >= 60000].index)\n",
    "data = data.drop(data[data[\"title length\"] >= 200].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7171c98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replotting title length vs text length to check impact of dropping outliers\n",
    "x = data[\"text length\"]\n",
    "y = data[\"title length\"]\n",
    "label = data[\"Class\"]\n",
    "colors = ['red','green']\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.scatter(x, y, c=label, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "plt.title(\"title length & text length vs. class\")\n",
    "plt.xlabel(\"text length\")\n",
    "plt.ylabel(\"title length\")\n",
    "\n",
    "cb = plt.colorbar()\n",
    "loc = np.arange(0,max(label),max(label)/float(len(colors)))\n",
    "cb.set_ticks(loc)\n",
    "cb.set_ticklabels(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93c2316",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating histogram of length of words in news text\n",
    "ax = data.hist(column='text words len', bins=40, grid=False, figsize=(12,8), color='#86bf91', zorder=2, rwidth=0.9)\n",
    "ax = ax[0]\n",
    "for x in ax:\n",
    "    x.spines['right'].set_visible(False)\n",
    "    x.spines['top'].set_visible(False)\n",
    "    x.spines['left'].set_visible(False)\n",
    "    x.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\", labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n",
    "    vals = x.get_yticks()\n",
    "    for tick in vals:\n",
    "        x.axhline(y=tick, linestyle='dashed', alpha=0.4, color='#eeeeee', zorder=1)\n",
    "    x.set_title(\"\")\n",
    "    x.set_xlabel(\"Number of words in news text\", labelpad=20, weight='bold', size=12)\n",
    "    x.set_ylabel(\"Frequency\", labelpad=20, weight='bold', size=12)\n",
    "    x.yaxis.set_major_formatter(StrMethodFormatter('{x:,g}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0c14a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating histogram of length of words in news titles\n",
    "ax = data.hist(column='title words len', bins=40, grid=False, figsize=(12,8), color='#86bf91', zorder=2, rwidth=0.9)\n",
    "ax = ax[0]\n",
    "for x in ax:\n",
    "    x.spines['right'].set_visible(False)\n",
    "    x.spines['top'].set_visible(False)\n",
    "    x.spines['left'].set_visible(False)\n",
    "    x.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\", labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n",
    "    vals = x.get_yticks()\n",
    "    for tick in vals:\n",
    "        x.axhline(y=tick, linestyle='dashed', alpha=0.4, color='#eeeeee', zorder=1)\n",
    "    x.set_title(\"\")\n",
    "    x.set_xlabel(\"Number of words in news text\", labelpad=20, weight='bold', size=12)\n",
    "    x.set_ylabel(\"Frequency\", labelpad=20, weight='bold', size=12)\n",
    "    x.yaxis.set_major_formatter(StrMethodFormatter('{x:,g}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6cd3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot pie graph of real vs fake news\n",
    "plot = data.label.value_counts().plot(kind='pie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b545218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127d6b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Report 2 CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3710f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from matplotlib.ticker import StrMethodFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747e2966",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "colnames=['title', 'text', 'label'] \n",
    "data = pd.read_csv(\"./news.csv\",usecols=colnames)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f068e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new columns with the string and word lengths for the text and titles\n",
    "data[\"title length\"] = data.title.str.len()\n",
    "data[\"text length\"] = data.text.str.len()\n",
    "data['Class'] = np.where(data['label'] ==\"FAKE\", 0, 1)\n",
    "data['text words len']=data.text.str.split().str.len()\n",
    "data['title words len']=data.title.str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cc565b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning the data\n",
    "# remove the hashtags, mentions and unwanted characters.\n",
    "import re\n",
    "def  clean_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
    "    return df\n",
    "\n",
    "data_clean = clean_text(data, \"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f20dde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make letters lowercase\n",
    "data[\"text\"] = data[\"text\"].str.lower()\n",
    "data[\"title\"] = data[\"title\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ead87e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Stop words & Lemmatize\n",
    "import en_core_web_sm\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer, WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from string import punctuation\n",
    "\n",
    "nlp = en_core_web_sm.load() \n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop = set(stopwords.words('english'))\n",
    "punctuation = list(string.punctuation) #already taken care of with the cleaning function.\n",
    "stop.update(punctuation)\n",
    "w_tokenizer = WhitespaceTokenizer()\n",
    "def furnished(text):\n",
    "    final_text = []\n",
    "    for i in w_tokenizer.tokenize(text):\n",
    "       if i.lower() not in stop:\n",
    "           word = lemmatizer.lemmatize(i)\n",
    "           final_text.append(word.lower())\n",
    "    return \" \".join(final_text)\n",
    "\n",
    "data_clean.title = data_clean.title.apply(furnished)\n",
    "data_clean.title = data_clean.text.apply(furnished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e05e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating the data to check check Imbalance between the datasets\n",
    "data_clean_REAL=data_clean.loc[data_clean.label==\"REAL\"]\n",
    "data_clean_FAKE=data_clean.loc[data_clean.label==\"FAKE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf7bbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking imbalance in real dataset\n",
    "data_clean_REAL.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765e5ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking imbalance in fake dataset\n",
    "data_clean_FAKE.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da13abdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056d735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Report 3 CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c842b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Naive Bayes / SVM & KNN classifiers for the text attribute\n",
    "# Importing Libraries to be used\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "\n",
    "#import data\n",
    "colnames=['title', 'text', 'Class'] \n",
    "np.random.seed(500)\n",
    "Corpus = pd.read_csv(\"./cleandata.csv\",usecols=colnames)\n",
    "Corpus.head()\n",
    "\n",
    "#Convert attributes to string format and tokenize to separate the words\n",
    "Corpus['text'] = Corpus['text'].astype(str)\n",
    "Corpus['title'] = Corpus['title'].astype(str)\n",
    "Corpus['tokenizedtext'] = Corpus.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)\n",
    "Corpus['tokenizedtitle'] = Corpus.apply(lambda row: nltk.word_tokenize(row['title']), axis=1)\n",
    "\n",
    "#Split Data for training and testing\n",
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(Corpus['text'],Corpus['Class'],test_size=0.2)\n",
    "\n",
    "#Fit the training data to encoder\n",
    "Encoder = LabelEncoder()\n",
    "Train_Y = Encoder.fit_transform(Train_Y)\n",
    "Test_Y = Encoder.fit_transform(Test_Y)\n",
    "\n",
    "\n",
    "#Fit Tfidf vectorizer to train the data\n",
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_vect.fit(Corpus['text'])\n",
    "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
    "\n",
    "####\n",
    "\n",
    "import time\n",
    "\n",
    "#### Training Naive Bayes Classifier\n",
    "start_time = time.time()\n",
    "# fit the training dataset on the NB classifier\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(Train_X_Tfidf,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_Y)*100)\n",
    "print(\"--- %s seconds to train---\" % (time.time() - start_time))\n",
    "\n",
    "# Printing Classification report to obtain precision recall and f1 score\n",
    "print(classification_report(Test_Y, predictions_NB))\n",
    "\n",
    "\n",
    "# Plot confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "plot_confusion_matrix(Naive, Test_X_Tfidf, Test_Y)  \n",
    "plt.show()\n",
    "\n",
    "\n",
    "#### Training SVM Classifier\n",
    "start_time = time.time()\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Accuracy: \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"--- %s seconds to train---\" % (time.time() - start_time))\n",
    "\n",
    "# Printing Classification report to obtain precision recall and f1 score\n",
    "print(classification_report(Test_Y, predictions_SVM))\n",
    "\n",
    "# Plot confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "plot_confusion_matrix(SVM, Test_X_Tfidf, Test_Y)  \n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "####  Training SVM Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "start_time = time.time()\n",
    "KNN = KNeighborsClassifier(n_neighbors=1)\n",
    "KNN.fit(Train_X_Tfidf,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_KNN = KNN.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Accuracy: \",accuracy_score(predictions_KNN, Test_Y)*100)\n",
    "print(\"--- %s seconds to train---\" % (time.time() - start_time))\n",
    "\n",
    "# Plot confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "plot_confusion_matrix(KNN, Test_X_Tfidf, Test_Y)  \n",
    "plt.show()\n",
    "\n",
    "# Printing Classification report to obtain precision recall and f1 score\n",
    "print(classification_report(Test_Y, predictions_KNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112bffe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Naive Bayes / SVM & KNN classifiers for the title attribute\n",
    "# Importing Libraries to be used\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "\n",
    "#import data\n",
    "colnames=['title', 'text', 'Class'] \n",
    "np.random.seed(500)\n",
    "Corpus = pd.read_csv(\"./cleandata.csv\",usecols=colnames)\n",
    "Corpus.head()\n",
    "\n",
    "#Convert attributes to string format and tokenize to separate the words\n",
    "Corpus['text'] = Corpus['text'].astype(str)\n",
    "Corpus['title'] = Corpus['title'].astype(str)\n",
    "Corpus['tokenizedtext'] = Corpus.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)\n",
    "Corpus['tokenizedtitle'] = Corpus.apply(lambda row: nltk.word_tokenize(row['title']), axis=1)\n",
    "\n",
    "#Split Data for training and testing\n",
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(Corpus['title'],Corpus['Class'],test_size=0.2)\n",
    "\n",
    "#Fit the training data to encoder\n",
    "Encoder = LabelEncoder()\n",
    "Train_Y = Encoder.fit_transform(Train_Y)\n",
    "Test_Y = Encoder.fit_transform(Test_Y)\n",
    "\n",
    "\n",
    "#Fit Tfidf vectorizer to train the data\n",
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_vect.fit(Corpus['title'])\n",
    "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
    "\n",
    "####\n",
    "\n",
    "import time\n",
    "\n",
    "#### Training Naive Bayes Classifier\n",
    "start_time = time.time()\n",
    "# fit the training dataset on the NB classifier\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(Train_X_Tfidf,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_Y)*100)\n",
    "print(\"--- %s seconds to train---\" % (time.time() - start_time))\n",
    "\n",
    "# Printing Classification report to obtain precision recall and f1 score\n",
    "print(classification_report(Test_Y, predictions_NB))\n",
    "\n",
    "\n",
    "# Plot confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "plot_confusion_matrix(Naive, Test_X_Tfidf, Test_Y)  \n",
    "plt.show()\n",
    "\n",
    "\n",
    "#### Training SVM Classifier\n",
    "start_time = time.time()\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Accuracy: \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"--- %s seconds to train---\" % (time.time() - start_time))\n",
    "\n",
    "# Printing Classification report to obtain precision recall and f1 score\n",
    "print(classification_report(Test_Y, predictions_SVM))\n",
    "\n",
    "# Plot confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "plot_confusion_matrix(SVM, Test_X_Tfidf, Test_Y)  \n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "####  Training SVM Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "start_time = time.time()\n",
    "KNN = KNeighborsClassifier(n_neighbors=1)\n",
    "KNN.fit(Train_X_Tfidf,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_KNN = KNN.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Accuracy: \",accuracy_score(predictions_KNN, Test_Y)*100)\n",
    "print(\"--- %s seconds to train---\" % (time.time() - start_time))\n",
    "\n",
    "# Plot confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "plot_confusion_matrix(KNN, Test_X_Tfidf, Test_Y)  \n",
    "plt.show()\n",
    "\n",
    "# Printing Classification report to obtain precision recall and f1 score\n",
    "print(classification_report(Test_Y, predictions_KNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8700bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a LSTM classifier for the text column\n",
    "\n",
    "# Importing Libraries to be used\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re\n",
    "import tqdm\n",
    "import unicodedata\n",
    "\n",
    "seed = 85\n",
    "np.random.seed(seed)\n",
    "\n",
    "#import Data\n",
    "colnames=['title', 'text', 'Class'] \n",
    "dataset = pd.read_csv(\"./cleandata.csv\",usecols=colnames)\n",
    "dataset['text'] = dataset['text'].astype(str)\n",
    "dataset['title'] = dataset['title'].astype(str)\n",
    "\n",
    "# Splitting Data\n",
    "from sklearn import model_selection\n",
    "text = dataset['text'].values\n",
    "classes = dataset['Class'].values\n",
    "train_text, test_text, train_class, test_class = model_selection.train_test_split(dataset['text'],dataset['Class'],test_size=0.2)\n",
    "\n",
    "# Function to remove links \n",
    "def strip_html_tags(text):\n",
    "  soup = BeautifulSoup(text, \"html.parser\")\n",
    "  [s.extract() for s in soup(['iframe', 'script'])]\n",
    "  stripped_text = soup.get_text()\n",
    "  stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "  return stripped_text\n",
    "\n",
    "# Function to remove non ascii characters\n",
    "def remove_accented_chars(text):\n",
    "  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "  return text\n",
    "\n",
    "# Function to remove special character\n",
    "def pre_process_corpus(docs):\n",
    "  norm_docs = []\n",
    "  for doc in tqdm.tqdm(docs):\n",
    "    doc = strip_html_tags(doc)\n",
    "    doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "    doc = doc.lower()\n",
    "    doc = remove_accented_chars(doc)\n",
    "    doc = contractions.fix(doc)\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
    "    doc = re.sub(' +', ' ', doc)\n",
    "    doc = doc.strip()  \n",
    "    norm_docs.append(doc)\n",
    "  \n",
    "  return norm_docs\n",
    "\n",
    "#Pre process the training and testing data\n",
    "norm_train_text = pre_process_corpus(train_text)\n",
    "norm_test_text = pre_process_corpus(test_text)\n",
    "\n",
    "#Tokenize data\n",
    "t = Tokenizer(oov_token='<UNK>')\n",
    "t.fit_on_texts(norm_train_text)\n",
    "t.word_index['<PAD>'] = 0\n",
    "max([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), min([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), t.word_index['<UNK>']\n",
    "train_sequences = t.texts_to_sequences(norm_train_text)\n",
    "test_sequences = t.texts_to_sequences(norm_test_text)\n",
    "\n",
    "\n",
    "#Prepare data for training\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "X_train = sequence.pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = sequence.pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_train.shape, X_test.shape\n",
    "le = LabelEncoder()\n",
    "\n",
    "#There are two classes\n",
    "num_classes=2\n",
    "\n",
    "#Transform and fit data\n",
    "y_train = le.fit_transform(train_class)\n",
    "y_test = le.transform(test_class)\n",
    "\n",
    "VOCAB_SIZE = len(t.word_index)\n",
    "EMBED_SIZE = 300\n",
    "EPOCHS=2\n",
    "BATCH_SIZE=128\n",
    "\n",
    "#Import libraries to create LSTM Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "# Function to create RNN Model\n",
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[MAX_SEQUENCE_LENGTH])\n",
    "    layer = Embedding(VOCAB_SIZE,300,input_length=MAX_SEQUENCE_LENGTH)(inputs)\n",
    "    layer = LSTM(64)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model\n",
    "\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\n",
    "\n",
    "# Model Creation\n",
    "model.fit(X_train, y_train, \n",
    "          validation_split=0.1,\n",
    "          epochs=EPOCHS, \n",
    "          batch_size=BATCH_SIZE, \n",
    "          verbose=1)\n",
    "\n",
    "# Training of model\n",
    "from tensorflow import keras\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)\n",
    "history = model.fit(X_train, y_train, epochs=10, verbose=0, validation_split=0.1, callbacks=[early_stop])\n",
    "hist = pd.DataFrame(history.history)\n",
    "hist[\"epoch\"] = history.epoch\n",
    "rmse_final = np.sqrt(float(hist['val_loss'].tail(1)))\n",
    "print(f\"final root mean square error on validation set is {round(rmse_final, 3)}\")\n",
    "\n",
    "\n",
    "# Plotting learning curve\n",
    "def plot_history():\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.plot(hist['epoch'], hist['loss'], label='Train Error')\n",
    "    plt.plot(hist['epoch'], hist['val_loss'], label='Val Error')\n",
    "    plt.legend()\n",
    "    plt.ylim([0, 5])\n",
    "\n",
    "plot_history()\n",
    "\n",
    "# Evaluate Model\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "# Generate predictions\n",
    "predictions = model.predict(X_test)\n",
    "classes_x=np.argmax(predictions,axis=1)\n",
    "predictions[:10]\n",
    "\n",
    "# Assign classes for predictions\n",
    "predictions = [1 if item >= 0.5 else 0 for item in predictions]\n",
    "predictions[:10]\n",
    "\n",
    "# Create Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "labels = ['FAKE', 'REAL']\n",
    "print(classification_report(test_class, predictions))\n",
    "pd.DataFrame(confusion_matrix(test_class, predictions), index=labels, columns=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aa9065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a LSTM classifier for the title column\n",
    "\n",
    "# Importing Libraries to be used\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re\n",
    "import tqdm\n",
    "import unicodedata\n",
    "\n",
    "seed = 85\n",
    "np.random.seed(seed)\n",
    "\n",
    "#import Data\n",
    "colnames=['title', 'text', 'Class'] \n",
    "dataset = pd.read_csv(\"./cleandata.csv\",usecols=colnames)\n",
    "dataset['text'] = dataset['text'].astype(str)\n",
    "dataset['title'] = dataset['title'].astype(str)\n",
    "\n",
    "# Splitting Data\n",
    "from sklearn import model_selection\n",
    "title = dataset['title'].values\n",
    "classes = dataset['Class'].values\n",
    "train_title, test_title, train_class, test_class = model_selection.train_test_split(dataset['title'],dataset['Class'],test_size=0.2)\n",
    "\n",
    "# Function to remove links \n",
    "def strip_html_tags(text):\n",
    "  soup = BeautifulSoup(text, \"html.parser\")\n",
    "  [s.extract() for s in soup(['iframe', 'script'])]\n",
    "  stripped_text = soup.get_text()\n",
    "  stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "  return stripped_text\n",
    "\n",
    "# Function to remove non ascii characters\n",
    "def remove_accented_chars(text):\n",
    "  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "  return text\n",
    "\n",
    "# Function to remove special character\n",
    "def pre_process_corpus(docs):\n",
    "  norm_docs = []\n",
    "  for doc in tqdm.tqdm(docs):\n",
    "    doc = strip_html_tags(doc)\n",
    "    doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "    doc = doc.lower()\n",
    "    doc = remove_accented_chars(doc)\n",
    "    doc = contractions.fix(doc)\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
    "    doc = re.sub(' +', ' ', doc)\n",
    "    doc = doc.strip()  \n",
    "    norm_docs.append(doc)\n",
    "  \n",
    "  return norm_docs\n",
    "\n",
    "#Pre process the training and testing data\n",
    "norm_train_title = pre_process_corpus(train_title)\n",
    "norm_test_title = pre_process_corpus(test_title)\n",
    "\n",
    "#Tokenize data\n",
    "t = Tokenizer(oov_token='<UNK>')\n",
    "t.fit_on_texts(norm_train_title)\n",
    "t.word_index['<PAD>'] = 0\n",
    "max([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), min([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), t.word_index['<UNK>']\n",
    "train_sequences = t.texts_to_sequences(norm_train_title)\n",
    "test_sequences = t.texts_to_sequences(norm_test_title)\n",
    "\n",
    "\n",
    "#Prepare data for training\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "X_train = sequence.pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = sequence.pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_train.shape, X_test.shape\n",
    "le = LabelEncoder()\n",
    "\n",
    "#There are two classes\n",
    "num_classes=2\n",
    "\n",
    "#Transform and fit data\n",
    "y_train = le.fit_transform(train_class)\n",
    "y_test = le.transform(test_class)\n",
    "\n",
    "VOCAB_SIZE = len(t.word_index)\n",
    "EMBED_SIZE = 300\n",
    "EPOCHS=2\n",
    "BATCH_SIZE=128\n",
    "\n",
    "#Import libraries to create LSTM Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "# Function to create RNN Model\n",
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[MAX_SEQUENCE_LENGTH])\n",
    "    layer = Embedding(VOCAB_SIZE,300,input_length=MAX_SEQUENCE_LENGTH)(inputs)\n",
    "    layer = LSTM(64)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model\n",
    "\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\n",
    "\n",
    "# Model Creation\n",
    "model.fit(X_train, y_train, \n",
    "          validation_split=0.1,\n",
    "          epochs=EPOCHS, \n",
    "          batch_size=BATCH_SIZE, \n",
    "          verbose=1)\n",
    "\n",
    "# Training of model\n",
    "from tensorflow import keras\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)\n",
    "history = model.fit(X_train, y_train, epochs=10, verbose=0, validation_split=0.1, callbacks=[early_stop])\n",
    "hist = pd.DataFrame(history.history)\n",
    "hist[\"epoch\"] = history.epoch\n",
    "rmse_final = np.sqrt(float(hist['val_loss'].tail(1)))\n",
    "print(f\"final root mean square error on validation set is {round(rmse_final, 3)}\")\n",
    "\n",
    "\n",
    "# Plotting learning curve\n",
    "def plot_history():\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.plot(hist['epoch'], hist['loss'], label='Train Error')\n",
    "    plt.plot(hist['epoch'], hist['val_loss'], label='Val Error')\n",
    "    plt.legend()\n",
    "    plt.ylim([0, 5])\n",
    "\n",
    "plot_history()\n",
    "\n",
    "# Evaluate Model\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "# Generate predictions\n",
    "predictions = model.predict(X_test)\n",
    "classes_x=np.argmax(predictions,axis=1)\n",
    "predictions[:10]\n",
    "\n",
    "# Assign classes for predictions\n",
    "predictions = [1 if item >= 0.5 else 0 for item in predictions]\n",
    "predictions[:10]\n",
    "\n",
    "# Create Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "labels = ['FAKE', 'REAL']\n",
    "print(classification_report(test_class, predictions))\n",
    "pd.DataFrame(confusion_matrix(test_class, predictions), index=labels, columns=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ba6900",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a CNN Classifier using text column\n",
    "\n",
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re\n",
    "import tqdm\n",
    "import unicodedata\n",
    "seed = 85\n",
    "np.random.seed(seed)\n",
    "\n",
    "#import Data\n",
    "colnames=['title', 'text', 'Class'] \n",
    "dataset = pd.read_csv(\"./cleandata.csv\",usecols=colnames)\n",
    "dataset['text'] = dataset['text'].astype(str)\n",
    "dataset['title'] = dataset['title'].astype(str)\n",
    "\n",
    "# Split Data\n",
    "from sklearn import model_selection\n",
    "text = dataset['text'].values\n",
    "classes = dataset['Class'].values\n",
    "train_text, test_text, train_class, test_class = model_selection.train_test_split(dataset['text'],dataset['Class'],test_size=0.2)\n",
    "\n",
    "# Function to remove links \n",
    "def strip_html_tags(text):\n",
    "  soup = BeautifulSoup(text, \"html.parser\")\n",
    "  [s.extract() for s in soup(['iframe', 'script'])]\n",
    "  stripped_text = soup.get_text()\n",
    "  stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "  return stripped_text\n",
    "\n",
    "# Function to remove non ascii characters\n",
    "def remove_accented_chars(text):\n",
    "  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "  return text\n",
    "\n",
    "# Function to remove special character\n",
    "def pre_process_corpus(docs):\n",
    "  norm_docs = []\n",
    "  for doc in tqdm.tqdm(docs):\n",
    "    doc = strip_html_tags(doc)\n",
    "    doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "    doc = doc.lower()\n",
    "    doc = remove_accented_chars(doc)\n",
    "    doc = contractions.fix(doc)\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
    "    doc = re.sub(' +', ' ', doc)\n",
    "    doc = doc.strip()  \n",
    "    norm_docs.append(doc)\n",
    "  \n",
    "  return norm_docs\n",
    "\n",
    "#Pre process the training and testing data\n",
    "norm_train_text = pre_process_corpus(train_text)\n",
    "norm_test_text = pre_process_corpus(test_text)\n",
    "\n",
    "#Tokenize data\n",
    "t = Tokenizer(oov_token='<UNK>')\n",
    "t.fit_on_texts(norm_train_text)\n",
    "t.word_index['<PAD>'] = 0\n",
    "max([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), min([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), t.word_index['<UNK>']\n",
    "train_sequences = t.texts_to_sequences(norm_train_text)\n",
    "test_sequences = t.texts_to_sequences(norm_test_text)\n",
    "\n",
    "#Prepare data for training\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "X_train = sequence.pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = sequence.pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_train.shape, X_test.shape\n",
    "le = LabelEncoder()\n",
    "\n",
    "# There are 2 classes\n",
    "num_classes=2\n",
    "\n",
    "#Transform and fit data\n",
    "y_train = le.fit_transform(train_class)\n",
    "y_test = le.transform(test_class)\n",
    "\n",
    "VOCAB_SIZE = len(t.word_index)\n",
    "EMBED_SIZE = 300\n",
    "EPOCHS=2\n",
    "BATCH_SIZE=128\n",
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(VOCAB_SIZE, EMBED_SIZE, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Conv1D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Model Creation\n",
    "model.fit(X_train, y_train, \n",
    "          validation_split=0.1,\n",
    "          epochs=EPOCHS, \n",
    "          batch_size=BATCH_SIZE, \n",
    "          verbose=1)\n",
    "\n",
    "#Evaluating and plotting learning curve\n",
    "from tensorflow import keras\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)\n",
    "history = model.fit(X_train, y_train, epochs=10, verbose=0, validation_split=0.1, callbacks=[early_stop])\n",
    "hist = pd.DataFrame(history.history)\n",
    "hist[\"epoch\"] = history.epoch\n",
    "rmse_final = np.sqrt(float(hist['val_loss'].tail(1)))\n",
    "\n",
    "\n",
    "def plot_history():\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.plot(hist['epoch'], hist['loss'], label='Train Error')\n",
    "    plt.plot(hist['epoch'], hist['val_loss'], label='Val Error')\n",
    "    plt.legend()\n",
    "    plt.ylim([0, 0.5])\n",
    "\n",
    "plot_history()\n",
    "\n",
    "# Model Evaluation\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "#Creating the predictions\n",
    "predictions = model.predict(X_test)\n",
    "classes_x=np.argmax(predictions,axis=1)\n",
    "predictions[:10]\n",
    "\n",
    "#Assigning classes based on the predictions\n",
    "predictions = [1 if item >= 0.5 else 0 for item in predictions]\n",
    "predictions[:10]\n",
    "\n",
    "#Creating confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "labels = ['FAKE', 'REAL']\n",
    "print(classification_report(test_class, predictions))\n",
    "pd.DataFrame(confusion_matrix(test_class, predictions), index=labels, columns=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a608410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a CNN Classifier using title column\n",
    "\n",
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re\n",
    "import tqdm\n",
    "import unicodedata\n",
    "seed = 85\n",
    "np.random.seed(seed)\n",
    "\n",
    "#import Data\n",
    "colnames=['title', 'text', 'Class'] \n",
    "dataset = pd.read_csv(\"./cleandata.csv\",usecols=colnames)\n",
    "dataset['text'] = dataset['text'].astype(str)\n",
    "dataset['title'] = dataset['title'].astype(str)\n",
    "\n",
    "# Split Data\n",
    "from sklearn import model_selection\n",
    "title = dataset['text'].values\n",
    "classes = dataset['Class'].values\n",
    "train_title, test_title, train_class, test_class = model_selection.train_test_split(dataset['title'],dataset['Class'],test_size=0.2)\n",
    "\n",
    "# Function to remove links \n",
    "def strip_html_tags(text):\n",
    "  soup = BeautifulSoup(text, \"html.parser\")\n",
    "  [s.extract() for s in soup(['iframe', 'script'])]\n",
    "  stripped_text = soup.get_text()\n",
    "  stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "  return stripped_text\n",
    "\n",
    "# Function to remove non ascii characters\n",
    "def remove_accented_chars(text):\n",
    "  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "  return text\n",
    "\n",
    "# Function to remove special character\n",
    "def pre_process_corpus(docs):\n",
    "  norm_docs = []\n",
    "  for doc in tqdm.tqdm(docs):\n",
    "    doc = strip_html_tags(doc)\n",
    "    doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "    doc = doc.lower()\n",
    "    doc = remove_accented_chars(doc)\n",
    "    doc = contractions.fix(doc)\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
    "    doc = re.sub(' +', ' ', doc)\n",
    "    doc = doc.strip()  \n",
    "    norm_docs.append(doc)\n",
    "  \n",
    "  return norm_docs\n",
    "\n",
    "#Pre process the training and testing data\n",
    "norm_train_title = pre_process_corpus(train_title)\n",
    "norm_test_title = pre_process_corpus(test_title)\n",
    "\n",
    "#Tokenize data\n",
    "t = Tokenizer(oov_token='<UNK>')\n",
    "t.fit_on_texts(norm_train_title)\n",
    "t.word_index['<PAD>'] = 0\n",
    "max([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), min([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), t.word_index['<UNK>']\n",
    "train_sequences = t.texts_to_sequences(norm_train_title)\n",
    "test_sequences = t.texts_to_sequences(norm_test_title)\n",
    "\n",
    "#Prepare data for training\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "X_train = sequence.pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = sequence.pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_train.shape, X_test.shape\n",
    "le = LabelEncoder()\n",
    "\n",
    "# There are 2 classes\n",
    "num_classes=2\n",
    "\n",
    "#Transform and fit data\n",
    "y_train = le.fit_transform(train_class)\n",
    "y_test = le.transform(test_class)\n",
    "\n",
    "VOCAB_SIZE = len(t.word_index)\n",
    "EMBED_SIZE = 300\n",
    "EPOCHS=2\n",
    "BATCH_SIZE=128\n",
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(VOCAB_SIZE, EMBED_SIZE, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Conv1D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Model Creation\n",
    "model.fit(X_train, y_train, \n",
    "          validation_split=0.1,\n",
    "          epochs=EPOCHS, \n",
    "          batch_size=BATCH_SIZE, \n",
    "          verbose=1)\n",
    "\n",
    "#Evaluating and plotting learning curve\n",
    "from tensorflow import keras\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)\n",
    "history = model.fit(X_train, y_train, epochs=10, verbose=0, validation_split=0.1, callbacks=[early_stop])\n",
    "hist = pd.DataFrame(history.history)\n",
    "hist[\"epoch\"] = history.epoch\n",
    "rmse_final = np.sqrt(float(hist['val_loss'].tail(1)))\n",
    "\n",
    "\n",
    "def plot_history():\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.plot(hist['epoch'], hist['loss'], label='Train Error')\n",
    "    plt.plot(hist['epoch'], hist['val_loss'], label='Val Error')\n",
    "    plt.legend()\n",
    "    plt.ylim([0, 0.5])\n",
    "\n",
    "plot_history()\n",
    "\n",
    "# Model Evaluation\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "#Creating the predictions\n",
    "predictions = model.predict(X_test)\n",
    "classes_x=np.argmax(predictions,axis=1)\n",
    "predictions[:10]\n",
    "\n",
    "#Assigning classes based on the predictions\n",
    "predictions = [1 if item >= 0.5 else 0 for item in predictions]\n",
    "predictions[:10]\n",
    "\n",
    "#Creating confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "labels = ['FAKE', 'REAL']\n",
    "print(classification_report(test_class, predictions))\n",
    "pd.DataFrame(confusion_matrix(test_class, predictions), index=labels, columns=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b878f42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
